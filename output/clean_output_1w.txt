data creation finished
emote class names:  ['boring', 'funny', 'happy', 'mock', 'sad', 'spam', 'surprise', 'undefined']

Using Sequantial data features
training set shape:  (134046, 49) (134046, 8)
testing set shape:  (89365, 49) (89365, 8)
Epoch 1/100
 - 279s - loss: 1.1645 - acc: 0.6217
Epoch 2/100
 - 277s - loss: 0.8660 - acc: 0.7159
Epoch 3/100
 - 278s - loss: 0.7390 - acc: 0.7557
Epoch 4/100
 - 279s - loss: 0.6532 - acc: 0.7838
Epoch 5/100
 - 280s - loss: 0.5879 - acc: 0.8051
Epoch 6/100
 - 280s - loss: 0.5370 - acc: 0.8222
Epoch 7/100
 - 280s - loss: 0.4937 - acc: 0.8373
Epoch 8/100
 - 281s - loss: 0.4620 - acc: 0.8472
Epoch 9/100
 - 281s - loss: 0.4355 - acc: 0.8563
Epoch 10/100
 - 281s - loss: 0.4137 - acc: 0.8631
Epoch 11/100
 - 281s - loss: 0.3944 - acc: 0.8693
Epoch 12/100
 - 281s - loss: 0.3809 - acc: 0.8732
Epoch 13/100
 - 281s - loss: 0.3703 - acc: 0.8771
Epoch 14/100
 - 281s - loss: 0.3576 - acc: 0.8809
Epoch 15/100
 - 281s - loss: 0.3492 - acc: 0.8831
Epoch 16/100
 - 281s - loss: 0.3448 - acc: 0.8851
Epoch 17/100
 - 281s - loss: 0.3372 - acc: 0.8876
Epoch 18/100
 - 281s - loss: 0.3291 - acc: 0.8895
Epoch 19/100
 - 281s - loss: 0.3279 - acc: 0.8896
Epoch 20/100
 - 281s - loss: 0.3232 - acc: 0.8907
Epoch 21/100
 - 281s - loss: 0.3192 - acc: 0.8931
Epoch 22/100
 - 282s - loss: 0.3163 - acc: 0.8935
Epoch 23/100
 - 281s - loss: 0.3118 - acc: 0.8951
Epoch 24/100
 - 281s - loss: 0.3135 - acc: 0.8942
Epoch 25/100
 - 281s - loss: 0.3064 - acc: 0.8973
Epoch 26/100
 - 281s - loss: 0.3036 - acc: 0.8976
Epoch 27/100
 - 281s - loss: 0.3007 - acc: 0.8982
Epoch 28/100
 - 281s - loss: 0.3031 - acc: 0.8974
Epoch 29/100
 - 281s - loss: 0.2972 - acc: 0.8998
Epoch 30/100
 - 282s - loss: 0.2966 - acc: 0.9008
Epoch 31/100
 - 281s - loss: 0.2935 - acc: 0.9003
Epoch 32/100
 - 281s - loss: 0.2941 - acc: 0.9005
Epoch 33/100
 - 281s - loss: 0.2938 - acc: 0.9003
Epoch 34/100
 - 281s - loss: 0.2905 - acc: 0.9017
Epoch 35/100
 - 281s - loss: 0.2906 - acc: 0.9014
Epoch 36/100
 - 281s - loss: 0.2882 - acc: 0.9017
Epoch 37/100
 - 281s - loss: 0.2896 - acc: 0.9018
Epoch 38/100
 - 281s - loss: 0.2873 - acc: 0.9024
Epoch 39/100
 - 281s - loss: 0.2875 - acc: 0.9028
Epoch 40/100
 - 281s - loss: 0.2871 - acc: 0.9018
Epoch 41/100
 - 281s - loss: 0.2834 - acc: 0.9036
Epoch 42/100
 - 281s - loss: 0.2865 - acc: 0.9033
Epoch 43/100
 - 281s - loss: 0.2817 - acc: 0.9046
Epoch 44/100
 - 281s - loss: 0.2818 - acc: 0.9046
Epoch 45/100
 - 281s - loss: 0.2863 - acc: 0.9034
Epoch 46/100
 - 281s - loss: 0.2838 - acc: 0.9031
Epoch 47/100
 - 281s - loss: 0.2824 - acc: 0.9045
Epoch 48/100
 - 281s - loss: 0.2814 - acc: 0.9047
Epoch 49/100
 - 281s - loss: 0.2800 - acc: 0.9045
Epoch 50/100
 - 281s - loss: 0.2798 - acc: 0.9045
Epoch 51/100
 - 281s - loss: 0.2796 - acc: 0.9048
Epoch 52/100
 - 281s - loss: 0.2788 - acc: 0.9050
Epoch 53/100
 - 281s - loss: 0.2775 - acc: 0.9060
Epoch 54/100
 - 281s - loss: 0.2791 - acc: 0.9049
Epoch 55/100
 - 281s - loss: 0.2802 - acc: 0.9048
Epoch 56/100
 - 282s - loss: 0.2758 - acc: 0.9060
Epoch 57/100
 - 281s - loss: 0.2785 - acc: 0.9052
Epoch 58/100
 - 281s - loss: 0.2795 - acc: 0.9047
Epoch 59/100
 - 281s - loss: 0.2752 - acc: 0.9062
Epoch 60/100
 - 281s - loss: 0.2777 - acc: 0.9055
Epoch 61/100
 - 281s - loss: 0.2756 - acc: 0.9063
Epoch 62/100
 - 282s - loss: 0.2779 - acc: 0.9056
Epoch 63/100
 - 281s - loss: 0.2762 - acc: 0.9061
Epoch 64/100
 - 281s - loss: 0.2768 - acc: 0.9061
Epoch 65/100
 - 281s - loss: 0.2768 - acc: 0.9057
Epoch 66/100
 - 281s - loss: 0.2712 - acc: 0.9080
Epoch 67/100
 - 281s - loss: 0.2753 - acc: 0.9065
Epoch 68/100
 - 281s - loss: 0.2738 - acc: 0.9062
Epoch 69/100
 - 281s - loss: 0.2755 - acc: 0.9056
Epoch 70/100
 - 281s - loss: 0.2744 - acc: 0.9068
Epoch 71/100
 - 281s - loss: 0.2769 - acc: 0.9058
Epoch 72/100
 - 281s - loss: 0.2714 - acc: 0.9076
Epoch 73/100
 - 281s - loss: 0.2731 - acc: 0.9065
Epoch 74/100
 - 281s - loss: 0.2707 - acc: 0.9074
Epoch 75/100
 - 281s - loss: 0.2727 - acc: 0.9065
Epoch 76/100
 - 282s - loss: 0.2731 - acc: 0.9073
Epoch 77/100
 - 283s - loss: 0.2741 - acc: 0.9065
Epoch 78/100
 - 283s - loss: 0.2747 - acc: 0.9063
Epoch 79/100
 - 282s - loss: 0.2735 - acc: 0.9062
Epoch 80/100
 - 282s - loss: 0.2740 - acc: 0.9064
Epoch 81/100
 - 310s - loss: 0.2758 - acc: 0.9055
Epoch 82/100
 - 301s - loss: 0.2759 - acc: 0.9062
Epoch 83/100
 - 305s - loss: 0.2746 - acc: 0.9062
Epoch 84/100
 - 305s - loss: 0.2751 - acc: 0.9056
Epoch 85/100
 - 288s - loss: 0.2745 - acc: 0.9062
Epoch 86/100
 - 282s - loss: 0.2750 - acc: 0.9069
Epoch 87/100
 - 283s - loss: 0.2731 - acc: 0.9063
Epoch 88/100
 - 282s - loss: 0.2730 - acc: 0.9069
Epoch 89/100
 - 282s - loss: 0.2724 - acc: 0.9064
Epoch 90/100
 - 282s - loss: 0.2732 - acc: 0.9059
Epoch 91/100
 - 282s - loss: 0.2724 - acc: 0.9068
Epoch 92/100
 - 282s - loss: 0.2742 - acc: 0.9064
Epoch 93/100
 - 282s - loss: 0.2746 - acc: 0.9064
Epoch 94/100
 - 282s - loss: 0.2698 - acc: 0.9078
Epoch 95/100
 - 283s - loss: 0.2735 - acc: 0.9063
Epoch 96/100
 - 283s - loss: 0.2729 - acc: 0.9056
Epoch 97/100
 - 282s - loss: 0.2745 - acc: 0.9065
Epoch 98/100
 - 282s - loss: 0.2746 - acc: 0.9063
Epoch 99/100
 - 283s - loss: 0.2729 - acc: 0.9076
Epoch 100/100
 - 283s - loss: 0.2748 - acc: 0.9060
[0.45217644938379853, 0.9075476976451586]
Evaluating:
precision: 0.908
recall: 0.908
f1: 0.908
class precision recall f1 support
boring 0.870 0.835 0.852 1125
funny 0.886 0.861 0.873 10654
happy 0.870 0.823 0.846 7389
mock 0.927 0.952 0.939 44082
sad 0.895 0.878 0.886 6671
spam 0.928 0.932 0.930 5302
surprise 0.883 0.850 0.866 5788
undefined 0.878 0.865 0.872 8354


Using TFIDF-Bag of word features
training set shape:  (134046, 177321) 134046
testing set shape:  (89365, 177321) 89365

Linear SVM :
train time: 6.163s
test time: 0.019s
precision: 0.920
recall: 0.920
f1: 0.920

class precision recall f1 support
boring 0.952 0.824 0.883 1125
funny 0.932 0.852 0.890 10654
happy 0.939 0.814 0.872 7389
mock 0.902 0.979 0.939 44082
sad 0.949 0.894 0.921 6671
spam 0.955 0.936 0.945 5302
surprise 0.940 0.852 0.894 5788
undefined 0.937 0.866 0.900 8354

top features:
boring:
[major bo1] [subs shroud] [ppl thinking] [ot mililul] [10 ot] [ynk] [dead production] [delay are] [boring] [24k for]

funny:
[xd here] [123] [no spam] [major only] [fans cry] [subs mililul] [cz on] [ban all] [haha this] [aizy 11]

happy:
[gg was] [clown time] [go astralis] [so found] [subs tbh] [major now] [cs austrian] [are best] [uptime is] [uptime]

mock:
[help no] [thinking] [puggers to] [help nobody] [taz face] [luler can] [help china] [belly huge] [omega] [hello]

sad:
[da_stoned] [baby] [being late] [mooom] [smh] [dead boys] [babyrage] [polish check] [c9 face] [mom]

spam:
[the list] [dosia face] [game game] [pray] [list] [cloud cloud] [jordyx3] [orc] [sloth] [aizy btw]

surprise:
[krimz kill] [silver is] [c9 usa] [round lul] [cerq cant] [banned on] [problem stewie] [ak on] [major 8k] [not dead]

undefined:
[only 36k] [21k watching] [under 100k] [minglee] [real weeb] [weeb can] [here problem] [help senor] [right from] [weeb]


Multinomial Naive Bayes :
train time: 0.130s
test time: 0.020s
precision: 0.684
recall: 0.684
f1: 0.684

class precision recall f1 support
boring 1.000 0.035 0.067 1125
funny 0.910 0.421 0.576 10654
happy 0.947 0.329 0.489 7389
mock 0.621 0.991 0.764 44082
sad 0.934 0.354 0.514 6671
spam 0.894 0.700 0.785 5302
surprise 0.917 0.203 0.333 5788
undefined 0.916 0.385 0.542 8354

Multi-layer Perception :
Iteration 1, loss = 0.85181223
Iteration 2, loss = 0.19659290
Iteration 3, loss = 0.11857707
Iteration 4, loss = 0.10614331
Iteration 5, loss = 0.10040739
Iteration 6, loss = 0.09826060
Iteration 7, loss = 0.09742342
Iteration 8, loss = 0.09761157
Iteration 9, loss = 0.10054437
Iteration 10, loss = 0.10251631
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
train time: 69.438s
test time: 0.028s
precision: 0.919
recall: 0.919
f1: 0.919

class precision recall f1 support
boring 0.926 0.828 0.874 1125
funny 0.914 0.863 0.888 10654
happy 0.886 0.827 0.855 7389
mock 0.920 0.966 0.943 44082
sad 0.941 0.888 0.914 6671
spam 0.955 0.936 0.945 5302
surprise 0.925 0.867 0.895 5788
undefined 0.903 0.886 0.894 8354

